[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelos Matemáticos para Aprendizado de Máquina 2",
    "section": "",
    "text": "% reais\n% vetor\n\n\nApresentação\n???"
  },
  {
    "objectID": "10-primeiras-nocoes.html#fonte",
    "href": "10-primeiras-nocoes.html#fonte",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.1 Fonte",
    "text": "1.1 Fonte\nEste capítulo transcreve uma apresentação de slides que fiz sobre este assunto.\nBoa parte do material apresentado aqui vem do capítulo 10 do livro James et al. (2021), que também está disponível online.\nVocê também pode assistir a uma playlist com mais de 100 vídeos, sobre os assuntos do livro James et al. (2021), apresentados pelos próprios autores."
  },
  {
    "objectID": "10-primeiras-nocoes.html#sumário",
    "href": "10-primeiras-nocoes.html#sumário",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.2 Sumário",
    "text": "1.2 Sumário"
  },
  {
    "objectID": "10-primeiras-nocoes.html#o-que-é-uma-rede-neural",
    "href": "10-primeiras-nocoes.html#o-que-é-uma-rede-neural",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.3 O que é uma rede neural",
    "text": "1.3 O que é uma rede neural\nNeste diagrama, vemos um exemplo de rede neural com uma camada oculta.\nOutros exemplos podem ter quantidades diferentes de camadas e de nós por camada.\nNeste exemplo, os quatro nós à esquerda (\\(X_1\\) a \\(X_4\\)) representam simplesmente os valores dos dados de entrada da rede. Apesar de não acontecer nenhuma computação nestes nós, alguns livros consideram que estes nós formam a camada de entrada.\nOs cinco nós azuis na coluna do meio (\\(A_1\\) a \\(A_5\\)) formam a camada onde a maior parte da computação acontece. É a camada oculta.\nO nó cor-de-rosa à direita produz o valor de saída da rede. É a camada de saída. Nem sempre a camada de saída vai consistir de apenas um nó.\nEstes nós também são chamados de neurônios, ou simplesmente unidades.\n\n\n\n\n\nCada nó produz um valor. As setas indicam como os valores dos nós são usados nas computações.\nPor exemplo, o nó \\(A_1\\) calcula uma função dos quatro valores de entrada. Vamos chamar esta função de \\(h_1\\):\n\\[\nA_1 = h_1(\\overrightarrow{X}) = h_1(X_1, X_2, X_3, X_4)\n\\]\n(\\(\\overrightarrow{X}\\) significa um vetor de valores, aqui com \\(4\\) elementos.)\nA função \\(h_1\\) começa calculando a seguinte combinação linear dos elementos do vetor:\n\\[\nw_{10} + w_{11} X_1 + w_{12} X_2 + w_{13} X_3 + w_{14} X_4\n\\]\nOs coeficientes \\(w_{ij}\\) são chamados de pesos, ou parâmetros.\nVocê pode imaginar que, no diagrama, cada seta corresponde a um peso:\n\n\\(w_{11}\\) corresponde à seta de \\(X_1\\) para \\(A_1\\),\n\\(w_{12}\\) corresponde à seta de \\(X_2\\) para \\(A_1\\),\n\\(w_{13}\\) corresponde à seta de \\(X_3\\) para \\(A_1\\), e assim por diante.\n\nMas, aqui, a seta que corresponde a \\(w_{10}\\) não é mostrada. Alguns livros representariam este peso como uma seta que vem “do nada” para o nó \\(A_1\\). Outros livros representariam este peso como um valor dentro do nó \\(A_1\\).\nEste peso \\(w_{10}\\), que não multiplica nenhum dos dados de entrada, é chamado de viés (bias, em inglês).\nO resultado desta combinação linear é enviado para uma função \\(g\\), fazendo, então, com que a saída do nó \\(A_1\\) seja\n\\[\nA_1 = g\\;(w_{10} + w_{11} X_1 + w_{12} X_2 + w_{13} X_3 + w_{14} X_4)\n\\]\nou, usando notação de somatório, mais compacta,\n\\[\nA_1 = g\\left(\n  w_{10}\n  + \\sum_{j=1}^4 w_{1j} X_j\\right)\n\\]\nA função \\(g\\) é chamada de função de ativação, e costuma ser uma função não-linear. Mais adiante, vamos ver exemplos específicos de funções de ativação.\nSe a função \\(g\\) fosse linear, a rede neural seria equivalente a regressão linear, que é um modelo menos poderoso.\nOu seja, é a não-linearidade da função de ativação que faz com que uma rede neural possa computar funções que modelos lineares não são capazes de computar.\n\n\n\n\n\nOs outros nós da camada oculta, \\(A_2\\) a \\(A_5\\), computam funções parecidas com a função computada pelo nó \\(A_1\\), mas com outros pesos.\nOu seja, no geral, para cada nó \\(A_i\\),\n\\[\n\\begin{aligned}\nA_i &= g\\;(w_{i0} + w_{i1} X_1 + w_{i2} X_2 + w_{i3} X_3 + w_{i4} X_4) \\\\\n&= g\\left(\n  w_{i0}\n  + \\sum_{j=1}^4 w_{ij} X_j\\right)\n\\end{aligned}\n\\]\nA função de ativação \\(g\\) costuma ser a mesma para todos os nós de uma mesma camada.\nTerminada a computação da camada oculta, os valores de saída de \\(A_1\\) a \\(A_5\\) são enviados para o (único) nó da última camada, que produz \\(Y\\), o valor de saída da rede:\n\\[\n\\begin{aligned}\n  Y\n  &= \\beta_0 + \\beta_1 A_1 + \\beta_2 A_2 + \\beta_3 A_3 + \\beta_4 A_4 + \\beta_5 A_5 \\\\\n  &= \\beta_0 + \\sum_{k=1}^5 \\beta_k A_k\n\\end{aligned}\n\\]\nAqui, neste exemplo, \\(Y\\) é função linear das saídas dos nós da camada oculta, mas é comum que \\(Y\\) também seja resultado de uma função não-linear.\n\n\n\n\n\nResumindo, os valores dos nós são calculados da seguinte forma:\n\\[\n  \\begin{aligned}\n  A_i &= g\\left(w_{i0} + \\sum_{j=1}^4 w_{ij} X_j\\right) \\\\\n  Y &= \\beta_0 + \\sum_{k=1}^5 \\beta_k A_k\n  \\end{aligned}\n  \\]\nEsta rede tem \\(4\\) entradas, uma camada oculta com \\(5\\) unidades e função de ativação \\(g()\\), uma camada de saída com uma unidade (com função de ativação linear), e, como pesos (ou parâmetros), o seguinte conjunto de valores:\n\n\\(w_{ij},\\, 1 \\leq i \\leq 5,\\, 0 \\leq j \\leq 4\\) (\\(25\\) parâmetros),\n\\(\\beta_k,\\, 0 \\leq k \\leq 5\\) (\\(6\\) parâmetros).\n\nEsta rede tem, então, \\(31\\) parâmetros no total."
  },
  {
    "objectID": "10-primeiras-nocoes.html#exemplos-de-funções-de-ativação",
    "href": "10-primeiras-nocoes.html#exemplos-de-funções-de-ativação",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.4 Exemplos de funções de ativação",
    "text": "1.4 Exemplos de funções de ativação\nJá vimos que a função de ativação costuma ser não-linear.\nUm exemplo é a função sigmóide, que sempre retorna valores entre \\(0\\) e \\(1\\) (exclusive) e que tem o gráfico mostrado abaixo. Esta função “comprime”, por assim dizer, todos os números reais para o intervalo \\((0, 1)\\).\nHistoricamente, a função sigmóide foi uma das primeiras a ser usadas como função de ativação, mas atualmente damos preferência a outras, que são menos problemáticas. Mais adiante, vamos falar sobre isto.\nA função sigmóide é útil, por exemplo, quando o nó precisa retornar uma probabilidade (que é um valor entre \\(0\\) e \\(1\\)).\nOutro exemplo de função de ativação é a ReLU, bem mais simples de ser computada (e entendida). Se o argumento é negativo, a ReLU retorna zero; senão, retorna o próprio argumento. Atualmente, é uma das funções de ativação mais usadas em redes neurais.\n\n\n\n\n\nVocê pode achar que a ReLU, uma função que simplesmente zera os argumentos que estão abaixo de zero, é simples demais para aproximar funções lineares. Este exemplo mostra que não:\nVamos considerar a função quadrática \\(f(x) = x^2 - 21x + 104\\):\n\n\n\n\n\nComeçamos com uma função linear: \\(f_1(x) = x/3 - 10/3\\).\n\n\n\n\n\nA seguir, aplicamos ReLU a esta reta:\n\n\n\n\n\nAgora, computamos outra função linear: \\(f_2(x) = -x/3 + 3\\).\n\n\n\n\n\nAplicamos ReLU a esta reta também:\n\n\n\n\n\nSomamos as duas ReLUs obtidas:\n\n\n\n\n\nNão ficou muito próxima da função original.\nEm vez de somar as duas ReLUs, vamos calcular uma combinação linear delas:\n\n\n\n\n\nAgora, sim, o resultado aproxima a função \\(f\\) original.\nSe usássemos mais funções lineares e aplicássemos mais ReLUs a elas, a aproximação seria melhor ainda."
  },
  {
    "objectID": "10-primeiras-nocoes.html#exemplo-aproximando-uma-função-quadrática",
    "href": "10-primeiras-nocoes.html#exemplo-aproximando-uma-função-quadrática",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.5 Exemplo: aproximando uma função quadrática",
    "text": "1.5 Exemplo: aproximando uma função quadrática\nAgora, vamos seguir esta mesma idéia, mas deixando uma rede neural fazer o trabalho.\nEsta é uma rede neural com uma entrada \\(x\\), uma única camada oculta, com \\(200\\) neurônios, com função de ativação ReLU, e camada de saída com um único neurônio.\nCom os valores dos pesos definidos por um processo de treinamento que vamos abordar depois, esta rede é capaz de retornar, para diversos valores de \\(x\\) entre \\(-50\\) e \\(50\\), o valor de \\(x^2\\), com mostra o gráfico:"
  },
  {
    "objectID": "10-primeiras-nocoes.html#regressão-versus-classificação",
    "href": "10-primeiras-nocoes.html#regressão-versus-classificação",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.6 Regressão versus classificação",
    "text": "1.6 Regressão versus classificação\nOs exemplos que vimos até agora são de redes neurais que calculam uma função numérica das variáveis de entrada. O que estas redes fazem é chamado de regressão.\nUm outro tipo de rede neural serve para computar a qual classe pertencem os dados de entrada.\nPor exemplo, as entradas podem representar dados numéricos sobre um paciente a ser diagnosticado, e as saídas (aqui, três) são as probabilidades de o paciente pertencer às classes\n\n\\(C_1 = {}\\) saudável,\n\\(C_2 = {}\\) indefinido,\n\\(C_3 = {}\\) doente.\n\nEste tipo de rede faz o que chamamos de classificação.\nMais adiante, vamos ver outros exemplos."
  },
  {
    "objectID": "10-primeiras-nocoes.html#o-que-falta-ver",
    "href": "10-primeiras-nocoes.html#o-que-falta-ver",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.7 O que falta ver?",
    "text": "1.7 O que falta ver?\nA esta altura, você deve ter entendido o que é e para que serve uma rede neural.\nOs exemplos que vimos foram de redes “já prontas”, com os valores dos pesos já definidos para as tarefas que as redes deviam executar.\nUma pergunta importante, que não abordamos aqui, é a de como os valores destes pesos são escolhidos.\nEm um outro capítulo, então, vamos falar sobre o processo de treinamento de uma rede neural, que é o que define os valores dos pesos."
  },
  {
    "objectID": "10-primeiras-nocoes.html#exercícios",
    "href": "10-primeiras-nocoes.html#exercícios",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.8 Exercícios",
    "text": "1.8 Exercícios\n\nNo diagrama do nosso primeiro exemplo de rede neural, escreva, próximo a cada seta, o nome do peso correspondente:\n\n\n\n\n\nLembre-se das equações:\n\n\\(A_i = g\\;(w_{i0} + w_{i1} X_1 + w_{i2} X_2 + w_{i3} X_3 + w_{i4} X_4)\\)\n\\(Y = \\beta_0 + \\beta_1 A_1 + \\beta_2 A_2 + \\beta_3 A_3 + \\beta_4 A_4 + \\beta_5 A_5\\)\n\nComo você decidiu representar os pesos \\(w_{i0}\\) e o peso \\(\\beta_0\\) (os chamados vieses ou biases)?\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, e Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. 2.ª ed. Springer Publishing Company, Incorporated. https://www.statlearning.com/."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "James, Gareth, Daniela Witten, Trevor Hastie, e Robert Tibshirani. 2021.\nAn Introduction to Statistical Learning: With Applications in\nR. 2.ª ed. Springer Publishing Company, Incorporated. https://www.statlearning.com/."
  }
]