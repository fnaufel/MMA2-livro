[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelos Matemáticos para Aprendizado de Máquina 2",
    "section": "",
    "text": "% reais\n% vetor\n\n\nApresentação\n???"
  },
  {
    "objectID": "10-primeiras-nocoes.html#fonte",
    "href": "10-primeiras-nocoes.html#fonte",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.1 Fonte",
    "text": "1.1 Fonte\nEste capítulo transcreve uma apresentação de slides que fiz sobre este assunto.\nBoa parte do material apresentado aqui vem do capítulo 10 do livro James et al. (2021), que também está disponível online.\nAlém disso, você pode assistir a uma playlist com mais de 100 vídeos, sobre os assuntos do livro, apresentados pelos próprios autores."
  },
  {
    "objectID": "10-primeiras-nocoes.html#sumário",
    "href": "10-primeiras-nocoes.html#sumário",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.2 Sumário",
    "text": "1.2 Sumário"
  },
  {
    "objectID": "10-primeiras-nocoes.html#o-que-é-uma-rede-neural",
    "href": "10-primeiras-nocoes.html#o-que-é-uma-rede-neural",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.3 O que é uma rede neural",
    "text": "1.3 O que é uma rede neural\nNeste diagrama, vemos uma rede neural com uma camada oculta.\nOs quatro nós à esquerda (\\(X_1\\) a \\(X_4\\)) representam simplesmente os valores dos dados de entrada da rede.\nOs cinco nós azuis na coluna do meio (\\(A_1\\) a \\(A_5\\)) formam a camada onde a maior parte da computação acontece.\nO nó cor-de-rosa à direita produz o valor de saída da rede.\n\n\n\n\n\nAs setas indicam como os valores dos nós são usados nas computações.\nPor exemplo, o nó \\(A_1\\) calcula uma função dos quatro valores de entrada. Vamos chamar esta função de \\(h_1\\):\n\\[\nA_1 = h_1(\\overrightarrow{X}) = h_1(X_1, X_2, X_3, X_4)\n\\]\n(\\(\\overrightarrow{X}\\) significa um vetor de valores, aqui com \\(4\\) elementos.)\nA função \\(h_1\\) começa calculando uma combinação linear dos elementos do vetor\n\\[\nw_{10} + w_{11} X_1 + w_{12} X_2 + w_{13} X_3 + w_{14} X_4\n\\]\nOs coeficientes \\(w_{ij}\\) são chamados de pesos, ou parâmetros.\nVocê pode imaginar que, no diagrama, cada seta corresponde a um peso:\n\n\\(w_{11}\\) corresponde à seta de \\(X_1\\) para \\(A_1\\),\n\\(w_{12}\\) corresponde à seta de \\(X_2\\) para \\(A_1\\),\n\\(w_{13}\\) corresponde à seta de \\(X_3\\) para \\(A_1\\), e assim por diante.\n\n(Mas, aqui, a seta que corresponde a \\(w_{10}\\) não é mostrada. Algumas fontes representariam este peso como uma seta que vem “do nada” para o nó \\(A_1\\).)\nO resultado desta combinação linear é enviado para uma função \\(g\\), fazendo, então, com que a saída do nó \\(A_1\\) seja\n\\[\nA_1 = g\\;(w_{10} + w_{11} X_1 + w_{12} X_2 + w_{13} X_3 + w_{14} X_4)\n\\]\nou, usando notação de somatório, mais compacta,\n\\[\nA_1 = g\\left(\n  w_{10}\n  + \\sum_{j=1}^4 w_{1j} X_j\\right)\n\\]\nA função \\(g\\) é chamada de função de ativação, e costuma ser uma função não-linear, como, por exemplo, a função sigmóide.\nA idéia é que, se a função \\(g\\) fosse linear, a rede neural seria equivalente a regressão linear.\nOu seja, é a não-linearidade da função de ativação que faz com que uma rede neural possa computar funções que outros modelos não são capazes de computar.\n\n\n\n\n\nOs outros nós da camada oculta, \\(A_2\\) a \\(A_5\\), computam funções parecidas com a função computada pelo nó \\(A_1\\), mas com outros pesos.\nOu seja, no geral, para cada nó \\(A_i\\),\n\\[\n\\begin{aligned}\nA_i &= g\\;(w_{i0} + w_{i1} X_1 + w_{i2} X_2 + w_{i3} X_3 + w_{i4} X_4) \\\\\n&= g\\left(\n  w_{i0}\n  + \\sum_{j=1}^4 w_{ij} X_j\\right)\n\\end{aligned}\n\\]\n(A função de ativação \\(g\\) costuma ser a mesma para todos os nós de uma mesma camada.)\nTerminada a computação da camada oculta, os valores de saída de \\(A_1\\) a \\(A_5\\) são enviados para o (único) nó da última camada, que produz \\(Y\\), o valor de saída da rede:\n\\[\n\\begin{aligned}\n  Y\n  &= \\beta_0 + \\beta_1 A_1 + \\beta_2 A_2 + \\beta_3 A_3 + \\beta_4 A_4 + \\beta_5 A_5 \\\\\n  &= \\beta_0 + \\sum_{k=1}^5 \\beta_k A_k\n\\end{aligned}\n\\]\nAqui, neste exemplo, \\(Y\\) é função linear das saídas dos nós da camada oculta, mas é comum que \\(Y\\) também seja não-linear.\n\n\n\n\n\nNesta computação, os \\(\\beta_k\\) também são chamados de pesos, ou parâmetros.\nPara esta rede, o conjunto de parâmetros consiste de\n\n\\(w_{ij},\\, 1 \\leq i \\leq 5,\\, 0 \\leq j \\leq 4\\) (\\(25\\) parâmetros),\n\\(\\beta_k,\\, 0 \\leq k \\leq 5\\) (\\(6\\) parâmetros).\n\nEsta rede tem, no total, \\(31\\) parâmetros."
  },
  {
    "objectID": "10-primeiras-nocoes.html#exemplos-de-funções-de-ativação",
    "href": "10-primeiras-nocoes.html#exemplos-de-funções-de-ativação",
    "title": "1  Primeiras noções sobre redes neurais",
    "section": "1.4 Exemplos de funções de ativação",
    "text": "1.4 Exemplos de funções de ativação\nJá vimos que a função de ativação costuma ser não-linear.\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, e Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. 2.ª ed. Springer Publishing Company, Incorporated. https://www.statlearning.com/."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "James, Gareth, Daniela Witten, Trevor Hastie, e Robert Tibshirani. 2021.\nAn Introduction to Statistical Learning: With Applications in\nR. 2.ª ed. Springer Publishing Company, Incorporated. https://www.statlearning.com/."
  }
]